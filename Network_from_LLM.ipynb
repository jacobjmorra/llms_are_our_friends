{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb7fe8-e9f4-4a87-b83b-973297ecfea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Read from the same movie text files (X lines, 5000 for now but can specify)\n",
    "2) Create a prompt for an LLM which tells the LLM to generate a heirarchy of movies based on their script\n",
    "3) Use the transformers .from_pretrained method with mistralai/Mistral-7B-v0.1, to feed the prompt to this model, and have it generate a heirarchy\n",
    "4) Then, create a graph based on this heirarchy, and compare the graph created by the LLM to the one created manually\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914aae02-4da5-4458-92a4-2441c1a7319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is the point? What can we do?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What is the point? What can we do?\n",
    "1) Discover which features/words from graphs are most salient for deciding categories\n",
    "2) We really want to understand how an LLM will understand concepts.. how to do this?\n",
    "3) E.g. we can probe the attention heads and create movie representations..??\n",
    "4) E.g. we can do the same when it is looking at the movie reviews.. how do the spaces look?\n",
    "5) Beyond just comparing how they look, need a way of putting labels on movies back..\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bbdbdf4-2123-446e-98dd-0c8b10244e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jacob\\appdata\\roaming\\jupyterlab-desktop\\envs\\fconn-holo\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.52.2-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 25.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.5 MB 26.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 27.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.31.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a11ee6-a9fc-448d-b9fa-bf75c3e69170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hugging face...\n",
    "llama_api_code = \"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\"\n",
    "llama_model_name = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1df124-9a9a-45e3-9c67-ea54f47f064f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52be9ad9c7c843609b840b18e22a278a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32a7a55c068644528107e3e541574e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698d263daaa34c7ca7be53b98f7c7b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9844565e4945878c9e8826f7c36ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc35d5150f54eb2b1011fd9cf59703b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c50ae79c8647adb416a476125eca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# llm_hierarchy_graph.py\n",
    "# Build script-similarity graph, ask Llama-2 for a hierarchy, compare & plot.\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# ─── AUTH ────────────────────────────────────────────────────────────────────\n",
    "HF_TOKEN = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────\n",
    "SCRIPT_DIR  = \"Action/Action\"\n",
    "MAX_LINES   = 5000\n",
    "SIM_THRESH  = 0.25\n",
    "TOP_K       = 5\n",
    "MODEL_NAME  = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "MAX_TOKENS  = 512\n",
    "TEMPERATURE = 0.7\n",
    "\n",
    "# ─── STEP 1: load scripts & build similarity graph ───────────────────────────\n",
    "paths  = sorted(glob.glob(os.path.join(SCRIPT_DIR, \"*.txt\")))\n",
    "titles = []\n",
    "docs   = []\n",
    "\n",
    "for p in paths:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        docs.append(\" \".join(itertools.islice(f, MAX_LINES)))\n",
    "    titles.append(os.path.splitext(os.path.basename(p))[0])\n",
    "\n",
    "# TF-IDF + cosine\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.9, stop_words=\"english\")\n",
    "X     = tfidf.fit_transform(docs)\n",
    "sim   = cosine_similarity(X)\n",
    "\n",
    "sim_graph = nx.Graph()\n",
    "for i, title in enumerate(titles, start=1):\n",
    "    sim_graph.add_node(i, title=title)\n",
    "\n",
    "n = len(titles)\n",
    "for i in range(n):\n",
    "    if TOP_K:\n",
    "        neighs = sim[i].argsort()[-(TOP_K+1):][::-1]\n",
    "    else:\n",
    "        neighs = [j for j in range(n) if j != i and sim[i,j] >= SIM_THRESH]\n",
    "\n",
    "    for j in neighs:\n",
    "        if i == j:\n",
    "            continue\n",
    "        w = float(sim[i,j])\n",
    "        if w < SIM_THRESH:\n",
    "            continue\n",
    "        sim_graph.add_edge(i+1, j+1, weight=w)\n",
    "\n",
    "# ─── STEP 2: build LLM prompt ────────────────────────────────────────────────\n",
    "prompt = (\n",
    "    \"You are an expert film critic.\\n\"\n",
    "    \"Produce a HIERARCHICAL clustering (indented plain text) of these ACTION movies.\\n\\n\"\n",
    ")\n",
    "for title, doc in zip(titles, docs):\n",
    "    snippet = doc[:1000].replace(\"\\n\", \" \")\n",
    "    prompt += f\"--- {title} ---\\n{snippet}\\n\\n\"\n",
    "prompt += \"Hierarchy:\\n\"\n",
    "\n",
    "# ─── STEP 3: load LLM & generate ─────────────────────────────────────────────\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_id  = 0      if torch.cuda.is_available() else -1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    token=HF_TOKEN,\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_auth_token=HF_TOKEN\n",
    ").to(device_str)\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=MAX_TOKENS,\n",
    "    do_sample=True,\n",
    "    temperature=TEMPERATURE,\n",
    "    device=device_id\n",
    ")\n",
    "\n",
    "generated = gen(prompt, return_full_text=False)[0][\"generated_text\"]\n",
    "print(\"=== LLM hierarchy ===\\n\")\n",
    "print(generated)\n",
    "\n",
    "# ─── STEP 4: parse hierarchy into DiGraph ───────────────────────────────────\n",
    "hier_graph = nx.DiGraph()\n",
    "stack = [(0, None)]   # (indent_level, parent_id)\n",
    "\n",
    "for line in generated.splitlines():\n",
    "    if not line.strip(): \n",
    "        continue\n",
    "    indent = len(line) - len(line.lstrip())\n",
    "    name   = line.strip(\"-•*0123456789. \").strip()\n",
    "    if not name:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        idx = titles.index(name) + 1\n",
    "    except ValueError:\n",
    "        idx = f\"group:{name}\"\n",
    "\n",
    "    # find correct parent\n",
    "    while stack and indent <= stack[-1][0]:\n",
    "        stack.pop()\n",
    "    parent = stack[-1][1]\n",
    "\n",
    "    hier_graph.add_node(idx)\n",
    "    if parent is not None:\n",
    "        hier_graph.add_edge(parent, idx)\n",
    "    stack.append((indent, idx))\n",
    "\n",
    "# ─── STEP 5: compare graphs ─────────────────────────────────────────────────\n",
    "ged = nx.graph_edit_distance(sim_graph, hier_graph, timeout=10)\n",
    "print(f\"\\nApproximate graph-edit distance: {ged}\")\n",
    "\n",
    "# ─── STEP 6: visualize both graphs ──────────────────────────────────────────\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# similarity graph\n",
    "plt.subplot(1,2,1)\n",
    "pos1 = nx.spring_layout(sim_graph, k=0.5, seed=42, weight=\"weight\")\n",
    "nx.draw(\n",
    "    sim_graph, pos1,\n",
    "    with_labels=True,\n",
    "    labels={i:i for i in sim_graph.nodes()},\n",
    "    node_color=\"#1f77b4\",\n",
    "    node_size=400,\n",
    "    font_color=\"white\"\n",
    ")\n",
    "plt.title(\"Script-similarity graph\")\n",
    "\n",
    "# LLM hierarchy\n",
    "plt.subplot(1,2,2)\n",
    "try:\n",
    "    pos2 = nx.nx_agraph.graphviz_layout(hier_graph, prog=\"dot\")\n",
    "except:\n",
    "    pos2 = nx.spring_layout(hier_graph, seed=42)\n",
    "nx.draw(\n",
    "    hier_graph, pos2,\n",
    "    with_labels=True,\n",
    "    node_color=\"#d62728\",\n",
    "    node_size=400\n",
    ")\n",
    "plt.title(\"LLM-generated hierarchy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b35b5a-358f-40ba-b7cd-ecc68fecb5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
